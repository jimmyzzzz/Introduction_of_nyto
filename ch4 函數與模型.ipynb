{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfc529a-bd4b-4ab5-8427-b0fdb84b59af",
   "metadata": {},
   "source": [
    "# ch4 函數與模型\n",
    "\n",
    "使用節點可以組織出複雜的運算系統，但節點也只是起了連接網路內各個元件的功能而已。支撐起整套運算系統的核心是模型和函數，下面我們就來簡單介紹一下兩者的差別:\n",
    "\n",
    "**`函數:`**\n",
    "1. 定義在unit_function模組中，功能是提供一些常見的函數，比如:tanh或是cross_entropy。\n",
    "2. 使用時不需要事先導入到網路，直接在建立連接時使用即可。\n",
    "3. 函數沒有需要優化的參數所以屬於資料節點。\n",
    "\n",
    "**`模型:`**\n",
    "1. 定義在layer模組中，功能是提供一些常見的(單層)類神經網路模型，比如:CNN或LSTM\n",
    "2. 使用時需要導入到網路，否則運行節點時會保錯。\n",
    "3. 模型需要優化的參數所以屬於模型節點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a528c4-4536-41b8-be4c-549700807f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyto import unit_function as uf\n",
    "from nyto import layer\n",
    "from nyto import net_tool as to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ffe3b9-fb81-4a6c-a6f0-c32a2d5f32fa",
   "metadata": {},
   "source": [
    "## 函數\n",
    "\n",
    "函數有嚴格的使用規定，主要分成支援右移運算和不支援右移運算兩種，支援右移運算的可以寫成下面兩種形式:\n",
    "\n",
    "    [1] node1 >> function(param,) >> node2\n",
    "    [2] node2 = function(param,)(node1)\n",
    "\n",
    "不支援右移運算只能寫成下面這種形式:\n",
    "    \n",
    "    [1] function(node1, param,) >> node2\n",
    "    [2] node2 = function(node1, param,)\n",
    "    \n",
    "通常不支援右移運算的函數都是需要輸入兩個或兩個節點以上的函數，所以無法支援右移運算。\n",
    "\n",
    "**`支援右移運算:`**\n",
    "* variable_to_np()\n",
    "* linear()\n",
    "* relu()\n",
    "* gaussian()\n",
    "* sigmoid()\n",
    "* tanh()\n",
    "* col_nor()\n",
    "* row_nor()\n",
    "* softmax()\n",
    "* global_average_pooling()\n",
    "* global_max_pooling()\n",
    "* flattening()\n",
    "* max_pooling(kernel_shape_node_if, strides=1)\n",
    "* average_pooling(kernel_shape_node_if, strides=1)\n",
    "\n",
    "**`不支援右移運算:`**\n",
    "* to_np(node_if)\n",
    "* concatenate(*node_ifs, axis=1)\n",
    "* tile(node_if, size_tuple)\n",
    "* MSE(pre_if, target_if)\n",
    "* RMSE(pre_if, target_if)\n",
    "* MAE(pre_if, target_if)\n",
    "* MAPE(pre_if, target_if)\n",
    "* cross_entropy(pre_if, target_if)\n",
    "* binary_cross_entropy(pre_if, target_if)\n",
    "* accuracy(pre_if, target_if)\n",
    "\n",
    "由於大部份的函數使用上都算直覺，這邊就挑幾個說明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b669179-7366-47c1-9301-053827c5207c",
   "metadata": {},
   "source": [
    "### max_pooling和average_pooling\n",
    "\n",
    "這兩個函數是卷集神經網路會用到的pooling函數，需要調整的參數為:\n",
    "* (tuple/node_interface)kernel_shape_node_if: 窗口的尺寸，比如(3,3)，也可以輸入一個節點界面。\n",
    "* (int/node_interface)strides=1: 窗口移動的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16665026-2699-4213-8a4a-0e1180badad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nyto.net.node_interface at 0x7f88053ed110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nn,n = to.new_net(\n",
    "    # 輸入的圖片是4維的numpy.array\n",
    "    # [第幾筆資料][channel][圖片row][圖片col]\n",
    "    img_np = to.add_data(np.arange(2*3*5*5).reshape(2,3,5,5)),\n",
    "    kernal_shape=(3,3),\n",
    "    strides=1\n",
    ")\n",
    "\n",
    "n.img_np >> uf.max_pooling(n.kernal_shape, n.strides) >> n.max_pooling\n",
    "n.img_np >> uf.average_pooling((3,3), strides=1) >> n.average_pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845fe6c-eac3-44af-8340-0cd7b7c2dd0a",
   "metadata": {},
   "source": [
    "### concatenate和tile\n",
    "\n",
    "這兩個函數是用來拼接numpy.array用的可以對應到numpy的函數:\n",
    "* concatenate(*node_ifs, axis=1) -> np.concatenate(node_ifs, axis=axis)\n",
    "* tile(node_if, size_tuple) -> np.tile(node_if, size_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0825d7c6-bbb3-4ad3-8cad-1d023bcad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn,n = to.new_net(\n",
    "    data1 = np.arange(9).reshape(3,3),\n",
    "    data2 = np.arange(9,18).reshape(3,3),\n",
    "    size_tuple = (3,2)\n",
    ")\n",
    "\n",
    "n.data3 = uf.concatenate(n.data1, n.data2)\n",
    "n.data4 = uf.tile(n.data1, size_tuple=n.size_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0156f3ce-3a77-46d5-8a3a-fa2dbb7b7749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  9, 10, 11],\n",
       "       [ 3,  4,  5, 12, 13, 14],\n",
       "       [ 6,  7,  8, 15, 16, 17]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get(n.data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec8b7bd-a254-4422-bf85-fadcb0d8d452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 0, 1, 2],\n",
       "       [3, 4, 5, 3, 4, 5],\n",
       "       [6, 7, 8, 6, 7, 8],\n",
       "       [0, 1, 2, 0, 1, 2],\n",
       "       [3, 4, 5, 3, 4, 5],\n",
       "       [6, 7, 8, 6, 7, 8],\n",
       "       [0, 1, 2, 0, 1, 2],\n",
       "       [3, 4, 5, 3, 4, 5],\n",
       "       [6, 7, 8, 6, 7, 8]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get(n.data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c4813-4ac6-4a3f-9f25-1a61a8d57596",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 模型\n",
    "\n",
    "下面來介紹模型，這邊需要注意的是前面我們介紹過模型節點，所謂的模型節點就是裡面保存了模型的節點。而模型節點最大的特點就是模型是可以被優化的，不能被優化的我們一般會被放到資料節點中。\n",
    "\n",
    "這邊我們要做更近一步的說明了，首先:可以被優化的單元一定是模型，但模型不一定可以被優化。也就是說模型中其實也存在可以優化的模型與不能優化的模型。你可能會感到困惑，那資料與模型的差別在於什麼呢？\n",
    "\n",
    "模型與資料的真正定義是:\n",
    "> 可以被多個網路共享的單元為資料，各網路獨有的單元為模型\n",
    "\n",
    "這說明模型與資料的差別並不是簡單的能不能優化而已，而在模型中存在著可以被優化與不能被優化的模型，我們將可以優化的模型稱為層模型(layer)。層模型被定義在`layer`模組中，有以下幾個成員:\n",
    "1. variable_layer: 單純的矩陣，一般用於尋找最佳的輸入\n",
    "2. nn_layer:       單層的類神經網路\n",
    "3. lstm_layer:     單層的LSTM\n",
    "4. conv_layer:     單層卷集層，裡面有多個keranl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c643d1b-7dc6-4ae5-82b3-eb05c3ff4c64",
   "metadata": {},
   "source": [
    "### variable_layer\n",
    "\n",
    "作為一個可優化的矩陣，能做到的事情遠比看上去的多。基本上，另外3種的層模型都可以使用該層去做出來，只不過會犧牲執行的效率。可以使用*layer.new_variable_layer*函數產生，下面是各項參數的說明:\n",
    "\n",
    "`layer.new_variable_layer`\n",
    "\n",
    "**structure**\n",
    "    \n",
    "    生成矩陣的形狀，資料型別是tuple。\n",
    "    exp:(3,2)\n",
    "**init_values**\n",
    "    \n",
    "    亂數生成時常態分配的平均數，預設0。\n",
    "**random_size**\n",
    "    \n",
    "    亂數生成時常態分配的標準差,為None時則使用init_values作為預設值。\n",
    "    預設None。\n",
    "**dropout**\n",
    "\n",
    "    使用dropout訓練時是否啟用dropout，預設False。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ebea8-45b3-4064-a43d-84647f647167",
   "metadata": {},
   "source": [
    "如果你手邊有一個已經準備好的`numpy.array`，你可以使用*layer.np_to_variable_layer*轉換成`variable_layer`，下面是該函數的參數:\n",
    "\n",
    "`layer.np_to_variable_layer`\n",
    "\n",
    "**variable_np**\n",
    "    \n",
    "    要被轉換的numpy.array。    \n",
    "**dropout**\n",
    "\n",
    "    使用dropout訓練時是否啟用dropout，預設False。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c61ac-0afc-4ec5-b1c7-d3ab63c80045",
   "metadata": {},
   "source": [
    "需要注意的是`variable_layer`在網路中被執行時，如果需要與其他的不是層模型的模型互動時，建議先轉換成np.array。可以使用`unit_function.variable_to_np`或是`unit_function.to_np`做轉換:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b68b9f0-340d-4825-b00b-3d4b9cb1fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn,n = to.new_net(\n",
    "    var1_layer=layer.new_variable_layer((3,3), 1), \n",
    "    var2_layer=layer.new_variable_layer((3,3), 2),\n",
    "    nn_layer=layer.new_nn_layer((3,3)),\n",
    "    data_np=np.arange(9).reshape(3,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28cf0d8-75d2-4914-af79-9a7fe10eeea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(variable_layer([[3. 3. 3.]\n",
       "  [3. 3. 3.]\n",
       "  [3. 3. 3.]]),\n",
       " array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 與其他層模型互動時，不需要傳換\n",
    "n.interact_with_other_layer1 = n.var1_layer + n.var2_layer\n",
    "n.interact_with_other_layer2 = n.var1_layer >> n.nn_layer\n",
    "\n",
    "to.get(n.interact_with_other_layer1, n.interact_with_other_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b3f3080-6f1a-44a5-bb9d-9019a0f611e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]]),\n",
       " array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 轉換方式1\n",
    "n.use_conversion_1 = uf.to_np(n.var1_layer) + n.data_np\n",
    "\n",
    "# 轉換方式2\n",
    "n.use_conversion_2 = n.var1_layer >> uf.variable_to_np() >> n.nn_layer\n",
    "\n",
    "to.get(n.use_conversion_1, n.use_conversion_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1205b7-6425-44dc-80f7-0c6b5ba7b1b9",
   "metadata": {},
   "source": [
    "### nn_layer\n",
    "\n",
    "單層的神經網路，可以用來組合出複雜的網路結構，可以使用*layer.new_nn_layer*產生:\n",
    "\n",
    "`layer.new_nn_layer`\n",
    "\n",
    "**structure**\n",
    "\n",
    "    網路結構，資料型別是tuple。\n",
    "    第一格是網路的輸入大小，第一格是網路的輸出大小。\n",
    "    exp:(3,2)  \n",
    "**init_values**\n",
    "\n",
    "    亂數生成時常態分配的平均數，資料型別是tuple，預設為(0,0)。\n",
    "    第一格是網路權重的平均數，第二格是網路偏置的平均數。\n",
    "**random_size**\n",
    "\n",
    "    亂數生成時常態分配的標準差,為None時則使用init_values作為預設值。\n",
    "    第一格是網路權重的標準差，第二格是網路偏置的標準差。\n",
    "    預設為(None, None)。\n",
    "**dropout**\n",
    "    \n",
    "    使用dropout訓練時是否啟用dropout，預設True。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d0b75-5692-4ac2-b231-3985d28505ea",
   "metadata": {},
   "source": [
    "### lstm_layer\n",
    "\n",
    "單層的神經網路，可以用來對時間序列分析，可以使用*layer.new_lstm_layer*產生:\n",
    "\n",
    "`layer.new_random_lstm`\n",
    "\n",
    "**structure**\n",
    "\n",
    "    網路結構，資料型別是tuple。\n",
    "    第一格是網路的輸入大小，第二格是網路的輸出大小。\n",
    "    exp:(3,2)  \n",
    "**init_values**\n",
    "\n",
    "    亂數生成時常態分配的平均數，資料型別是tuple，預設為(0,0)。\n",
    "    第一格是網路權重的平均數，第二格是網路偏置的平均數。\n",
    "**random_size**\n",
    "\n",
    "    亂數生成時常態分配的標準差,為None時則使用init_values作為預設值。\n",
    "    第一格是網路權重的標準差，第二格是網路偏置的標準差。\n",
    "    預設為(None, None)。\n",
    "**dropout**\n",
    "    \n",
    "    使用dropout訓練時是否啟用dropout，預設True。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15357e9f-29b3-4031-8758-62059233eab4",
   "metadata": {},
   "source": [
    "### conv_layer\n",
    "\n",
    "單層的卷集層，可以用來對圖像資料分析，可以使用*layer.new_conv_layer*產生:\n",
    "\n",
    "`layer.new_random_lstm`\n",
    "\n",
    "**structure**\n",
    "\n",
    "    kernel結構，資料型別是tuple。\n",
    "    第一格是kernel數量，第二格是kernel的row大小,第三格是kernel的col大小。\n",
    "    exp:(2,3,3)\n",
    "    \n",
    "**init_values**\n",
    "\n",
    "    亂數生成時常態分配的平均數，預設0。\n",
    "**random_size**\n",
    "\n",
    "    亂數生成時常態分配的標準差,為None時則使用init_values作為預設值。\n",
    "    預設None。\n",
    "**dropout**\n",
    "    \n",
    "    使用dropout訓練時是否啟用dropout，資料型別是bool。\n",
    "    預設False。\n",
    "    \n",
    "**pad_mod**\n",
    "\n",
    "    padding模式，有三種選擇:(1)'full'(2)'valid'(3)'same'\n",
    "    'same'=不做padding，預設為'valid'\n",
    "    \n",
    "**strides**\n",
    "\n",
    "    窗口移動大小，資料型別是int。\n",
    "    預設為1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b689d-88bf-4368-b87a-151c71fb6c5c",
   "metadata": {},
   "source": [
    "## 子網路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad13c4-218c-4f1a-8f6d-50c07e2f3c2e",
   "metadata": {},
   "source": [
    "如果你手邊正好有一個訓練好的網路，它能不能與其他網路組合起來變成一個更好的網路呢？被其他網路導入的網路我們稱之為子網路，我們可以使用`net.push_get`來在其他網路中完成對該網路的使用:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a02d873-345a-47f4-9926-a990c279b8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 子網路\n",
    "sub_nn, sub_nn_node = to.new_net()\n",
    "sub_nn_node.y = sub_nn_node.x1 + sub_nn_node.x2\n",
    "\n",
    "# 導入子網路\n",
    "nn, node = to.new_net(a=1, b=2, sub_nn=sub_nn)\n",
    "node.sub_nn_return = node.sub_nn.push_get(\n",
    "    {'y'}, x1=node.a, x2=node.b\n",
    ")\n",
    "\n",
    "to.get(node.sub_nn_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cc62c-668e-40a9-bca3-0a6d824a19c2",
   "metadata": {},
   "source": [
    "對於子網路，我們應該要如何看待呢？可以簡單當成層模型來使用。當優化時，被導入的子網路也會如同網路內的其他層模型一樣被優化。但如果只想使用而不想優化子網路，最簡單的方法就是在導入時使用資料的方式導入。當然更簡單的方式也是有的，使用者可以很方便的切換是否需要優化子網路，我們在下一章中再做介紹。\n",
    "\n",
    "***\n",
    "\n",
    "*END*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
